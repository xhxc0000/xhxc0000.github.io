<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><title>基于Java的爬虫框架WebCollector</title><meta http-equiv="content-type" content="text/html; charset=utf-8"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css"><link rel="stylesheet" href="/css/progressjs.css"><script src="/js/progress.js"></script><script src="/js/jquery.min.js"></script><script src="/js/main.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-160006603-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-160006603-1');
</script><meta name="viewport" content="width=device-width, initial-scale=0.5"><link rel="stylesheet" href="/css/post.css"></head><body><script>if (document.readyState == 'loading') {
  progressJs().start()
}
document.addEventListener('readystatechange', (event) => {
  if (document.readyState == 'interactive') {
    progressJs().set(50)
  }
  if (document.readyState == 'complete') {
    progressJs().end()
  }
});
</script><div><div class="inner"><h2>基于Java的爬虫框架WebCollector</h2><p>Long, Long Ago，网络上出现大量Python爬虫教程，各种培训班借势宣扬Python，近几年又将噱头转向人工智能。爬虫是一个可以简单也可以复杂的概念，就好比建造狗屋和建筑高楼大厦都是在搞工程。</p>
<p>由于工作的缘故，我需要使用WebCollector爬取一些网页上的数据。其实宏观上，爬虫无非就是访问页面文件，把需要的数据提取出来，然后把数据储存到数据库里。难点往往在于，一是目标网站的反爬策略，这是让人比较无奈的斗智斗勇的过程；二是目标网页数量大、类型多，如何制定有效的数据爬取和数据分析方案。</p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>这是一张简略的概念图，受屏幕宽度限制，可能无法看清内容，请在新标签页打开图片，或者直接点击 <a href="/2019/08/10/%E5%9F%BA%E4%BA%8EJava%E7%9A%84%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6WebCollector/WebCollector.png">这里</a>。这张图片并不是完美的，甚至还包含不完全正确的实现方式，具体内容会在后面阐述。</p>
<p><img src="WebCollector.png" width="95%" height="100%"></p>
<p>我将目标网页分为4种类型：</p>
<ol>
<li>静态的网页文档，curl就可以加载到</li>
<li>需要自定义HTTP请求的页面，比如由POST请求得到的搜索结果页面，或者需要使用Cookie进行鉴权的页面</li>
<li>页面中包含由JavaScript生成的数据，而我们需要的正是这部分数据。由于js是加载后才执行的，就像CSS加载后由浏览器进行渲染一样，这样的数据无法直接得到</li>
<li>页面中包含由JavaScript生成的数据，且需要自定义HTTP请求的页面</li>
</ol>
<h3 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h3><p>为了便于测试，在本地使用Node.js启动一个简单的服务器，用于接收请求，并返回一个页面作为响应。server.js的内容如下：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> http = <span class="built_in">require</span>(<span class="string">'http'</span>)</span><br><span class="line"><span class="keyword">var</span> fs = <span class="built_in">require</span>(<span class="string">'fs'</span>)</span><br><span class="line"><span class="keyword">var</span> server = http.createServer(<span class="function">(<span class="params">req,res</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="comment">// 返回页面内容</span></span><br><span class="line">  fs.readFile(<span class="string">'./index.html'</span>, <span class="string">'utf-8'</span>, (err,data) =&gt; &#123;</span><br><span class="line">    res.end(data);</span><br><span class="line">  &#125;);</span><br><span class="line">  <span class="comment">// 打印请求中的Cookie信息</span></span><br><span class="line">  <span class="built_in">console</span>.log(req.headers.cookie)</span><br><span class="line">&#125;)</span><br><span class="line">server.listen(<span class="number">9000</span>)</span><br></pre></td></tr></table></figure>
<p>index.html的内容更加简单，只包含一个title和一个p标签：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="静态页面"><a href="#静态页面" class="headerlink" title="静态页面"></a>静态页面</h3><p>这是一个最简版的爬虫程序，在构造方法中调用父类的有参构造方法，同时添加url到待爬取队列中。visit是消费者，每一个url请求都会进入这个方法被处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StaticDocs</span> <span class="keyword">extends</span> <span class="title">BreadthCrawler</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StaticDocs</span><span class="params">(String crawlPath, <span class="keyword">boolean</span> autoParse)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(crawlPath, autoParse);</span><br><span class="line">        <span class="keyword">this</span>.addSeed(<span class="string">"http://127.0.0.1:9000/"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">visit</span><span class="params">(Page page, CrawlDatums next)</span> </span>&#123;</span><br><span class="line">        System.out.println(page.doc().title();</span><br><span class="line">        <span class="comment">// This is a title</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StaticDocs crawler = <span class="keyword">new</span> StaticDocs(<span class="string">"crawl"</span>, <span class="keyword">true</span>);</span><br><span class="line">        crawler.start(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Cookie鉴权"><a href="#Cookie鉴权" class="headerlink" title="Cookie鉴权"></a>Cookie鉴权</h3><p>需要在header中带cookie请求同样简单，在构造方法中添加相应配置就可以，node.js的命令行会打印出cookie的内容：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CookieDocs</span><span class="params">(String crawlPath)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(crawlPath, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置请求插件</span></span><br><span class="line">    setRequester(<span class="keyword">new</span> OkHttpRequester() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Request.<span class="function">Builder <span class="title">createRequestBuilder</span><span class="params">(CrawlDatum crawlDatum)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">super</span>.createRequestBuilder(crawlDatum)</span><br><span class="line">                    .header(<span class="string">"Cookie"</span>, <span class="string">"name=smallyu"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.addSeed(<span class="string">"http://127.0.0.1:9000/"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// name=smallyu</span></span><br></pre></td></tr></table></figure>
<h3 id="JavaScript生成的数据"><a href="#JavaScript生成的数据" class="headerlink" title="JavaScript生成的数据"></a>JavaScript生成的数据</h3><p>测试js生成数据的情况需要做一点准备，修改index.html，在body标签中加入这样几行代码：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;div id=<span class="string">"content"</span>&gt;<span class="number">1</span>&lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">&lt;script&gt;</span></span><br><span class="line"><span class="regexp">  document.getElementById('content').innerHTML = '2'</span></span><br><span class="line"><span class="regexp">&lt;/</span>script&gt;</span><br></pre></td></tr></table></figure>
<p>可以预见，请求中直接返回的div内容是1，然后js经由浏览器执行，改变div的内容为2。访问静态页面的爬虫程序只能进行到第1步，也就是直接获取请求返回的内容。修改StaticDocs.java的visit方法，打印出div的内容看一下，可以确信是1：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.out.println(page.select(<span class="string">"div"</span>).text());</span><br><span class="line"><span class="comment">// 1</span></span><br></pre></td></tr></table></figure>
<p>这是一个官方提供的Demo，用于获取js生成的数据。WebCollector依赖于Selenium，使用HtmlUnitDriver运行js：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsDocs</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Executor executor = (CrawlDatum datum, CrawlDatums next) -&gt; &#123;</span><br><span class="line">            HtmlUnitDriver driver = <span class="keyword">new</span> HtmlUnitDriver();</span><br><span class="line">            driver.setJavascriptEnabled(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">            driver.get(datum.url());</span><br><span class="line"></span><br><span class="line">            WebElement divEle = driver.findElement(By.id(<span class="string">"content"</span>));</span><br><span class="line">            System.out.println(divEle.getText());</span><br><span class="line">            <span class="comment">// 2</span></span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个基于伯克利DB的DBManager</span></span><br><span class="line">        DBManager manager = <span class="keyword">new</span> RocksDBManager(<span class="string">"crawl"</span>);</span><br><span class="line">        <span class="comment">//创建一个Crawler需要有DBManager和Executor</span></span><br><span class="line">        Crawler crawler = <span class="keyword">new</span> Crawler(manager, executor);</span><br><span class="line">        crawler.addSeed(<span class="string">"http://127.0.0.1:9000/"</span>);</span><br><span class="line">        crawler.start(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果你看过WebCollector的主页，一定可以注意到这个Demo和其他Demo的明显不同。在不需要js生成的数据时，新建的类继承自BreadthCrawler，而BreadthCrawler继承自AutoParseCrawler，AutoParseCrawler又继承自Crawler。现在获取js数据的Demo，直接跳过BreadthCrawler和AutoParseCrawler，实例化了Crawler。</p>
<p><img src="uml.png" width="50%" height="100%"></p>
<p>为什么要这样做呢？再次强调，这是官方提供的Demo。</p>
<h3 id="Cookie鉴权后JavaScript生成的数据"><a href="#Cookie鉴权后JavaScript生成的数据" class="headerlink" title="Cookie鉴权后JavaScript生成的数据"></a>Cookie鉴权后JavaScript生成的数据</h3><p>根据官方提供的用例，显然是无法设置cookie的，因为Crawler类并没有提供自定义Header的方法。这个自定义Header的方法继承自AutoParseCrawler类。那么如何做到既可以添加Cookie又可以使用HtmlUnitDriver？</p>
<p>其实结果很简单，我在看过WebCollector的代码后发现AutoParseCrawler实现了Executor接口，并且在构造方法中将this赋值给了父类的executor。也就是说，AutoParseCrawler本身就是一个Executor。下面的代码用以表示它们的关系：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Crawler</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> Executor executor;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Crawler</span><span class="params">(DBManager dbManager, Executor executor)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AutoParseCrawler</span> <span class="keyword">extends</span> <span class="title">Crawler</span> <span class="keyword">implements</span> <span class="title">Executor</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">AutoParseCrawler</span><span class="params">(<span class="keyword">boolean</span> autoParse)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 这里的executor指向父类</span></span><br><span class="line">        <span class="keyword">this</span>.executor = <span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>new Crawler时传入一个executor，相当于直接new一个AutoParseCrawler。BreadthCrawler继承自AutoParseCrawler，所以BreadthCrawler本身也是个Executor。再看官方关于自定义Cookie的Demo，如何在其中使用HtmlUnitDriver呢？重写Executor的execute方法。</p>
<p>所以，在定义cookie后获取js生成的数据，使用继承BreadthCrawler的类，然后重写execute就可以。这是一个完整的Demo：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> smallyu</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2019.08.11 12:18</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsWithCookieDocs</span> <span class="keyword">extends</span> <span class="title">BreadthCrawler</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">JsWithCookieDocs</span><span class="params">(String crawlPath)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(crawlPath, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置请求插件</span></span><br><span class="line">        setRequester(<span class="keyword">new</span> OkHttpRequester() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Request.<span class="function">Builder <span class="title">createRequestBuilder</span><span class="params">(CrawlDatum crawlDatum)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">super</span>.createRequestBuilder(crawlDatum)</span><br><span class="line">                        .header(<span class="string">"Cookie"</span>, <span class="string">"name=smallyu"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.addSeed(<span class="string">"http://127.0.0.1:9000/"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 直接重写execute即可</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(CrawlDatum datum, CrawlDatums next)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.execute(datum, next);</span><br><span class="line"></span><br><span class="line">        HtmlUnitDriver driver = <span class="keyword">new</span> HtmlUnitDriver();</span><br><span class="line">        driver.setJavascriptEnabled(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        driver.get(datum.url());</span><br><span class="line"></span><br><span class="line">        WebElement divEle = driver.findElement(By.id(<span class="string">"content"</span>));</span><br><span class="line">        System.out.println(divEle.getText());</span><br><span class="line">        <span class="comment">// 2</span></span><br><span class="line">        <span class="comment">// 同时，node.js的命令行中打印出cookie内容</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重写execute就不需要visit了</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">visit</span><span class="params">(Page page, CrawlDatums crawlDatums)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        JsWithCookieDocs crawler = <span class="keyword">new</span> JsWithCookieDocs(<span class="string">"crawl"</span>);</span><br><span class="line">        crawler.start(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="外部代理"><a href="#外部代理" class="headerlink" title="外部代理"></a>外部代理</h3><p>也许还没有结束。在一开始概述的图片上，同时定义cookie以及获取js生成的数据，实现方式是内部Selenium + 外部browsermob-proxy。假设没有上述重写execute的方法（官方也确实没有提供类似的Demo），该如何实现想要的效果？一种实践是本地启动一个代理，给代理设置好cookie，然后让Selenium的WebDriver通过代理访问目标页面，就可以在带header的情况下拿到js生成的数据。这是在JsDocs.java的基础上，使用代理的完整实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsWithProxyDocs</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Executor executor = (CrawlDatum datum, CrawlDatums next) -&gt; &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 启动一个代理</span></span><br><span class="line">            BrowserMobProxy proxy = <span class="keyword">new</span> BrowserMobProxyServer();</span><br><span class="line">            proxy.start(<span class="number">0</span>);</span><br><span class="line">            <span class="comment">// 添加header</span></span><br><span class="line">            proxy.addHeader(<span class="string">"Cookie"</span> , <span class="string">"name=smallyu"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 实例化代理对象</span></span><br><span class="line">            Proxy seleniumProxy = ClientUtil.createSeleniumProxy(proxy);</span><br><span class="line">            <span class="comment">// 由代理对象生成capabilities</span></span><br><span class="line">            DesiredCapabilities capabilities = <span class="keyword">new</span> DesiredCapabilities();</span><br><span class="line">            capabilities.setCapability(CapabilityType.PROXY, seleniumProxy);</span><br><span class="line">            <span class="comment">// 内置，必须设置</span></span><br><span class="line">            capabilities.setBrowserName(<span class="string">"htmlunit"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 使用capabilities实例化HtmlUnitDriver</span></span><br><span class="line">            HtmlUnitDriver driver = <span class="keyword">new</span> HtmlUnitDriver(capabilities);</span><br><span class="line">            driver.setJavascriptEnabled(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">            driver.get(datum.url());</span><br><span class="line"></span><br><span class="line">            WebElement divEle = driver.findElement(By.id(<span class="string">"content"</span>));</span><br><span class="line">            System.out.println(divEle.getText());   <span class="comment">// 2</span></span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个Crawler需要有DBManager和Executor</span></span><br><span class="line">        Crawler crawler = <span class="keyword">new</span> Crawler(<span class="keyword">new</span> RocksDBManager(<span class="string">"crawl"</span>), executor);</span><br><span class="line">        crawler.addSeed(<span class="string">"http://127.0.0.1:9000/"</span>);</span><br><span class="line">        crawler.start(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>对于WebCollector我已经没有兴趣了解更多，倒是在注意到框架的包名<code>cn.edu.hfut</code>后有种豁然开朗的感觉。凌乱的代码风格，随处可见不知所以的注释，毫无设计美感的代码架构，倒也符合国内不知名大学的开源软件水平，距离工业级的框架，可能还需要N个指数倍东的时间。至于使用过程中遇到depth含义不明、线程非法结束、next.add失效等问题，就这样吧，也在情理之中，整个框架都像是赶工的结果，或者说是学生们拿来练手的项目。我在WebCollector的Github上RP了关于重写execute的问题，从开发者回复的只言片语中，我怀疑开源者自己都没有把里面的东西搞清楚 :P</p>
<div class="align-right"><span>2019-08-10 21:03</span><span> 创建</span></div><div class="align-right"><span>2020-03-15 17:29</span><span> 更新</span></div></div></div></body></html>